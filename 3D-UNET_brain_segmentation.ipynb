{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled25.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVaKC/j6ymrdmE1WNLFEnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Article_implementation/blob/main/3D-UNET_brain_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Brain_segmentation_pytorch 3D-UNET**\n",
        "\n",
        "CTの複数画像を立体的にsegmentationする\n",
        "\n",
        "GitHub: hhttps://github.com/mateuszbuda/brain-segmentation-pytorch\n",
        "\n",
        "WebPage: https://towardsdatascience.com/creating-and-training-a-u-net-model-with-pytorch-for-2d-3d-semantic-segmentation-model-building-6ab09d6a0862\n"
      ],
      "metadata": {
        "id": "MTShhTbs_oB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mateuszbuda/brain-segmentation-pytorch.git\n",
        "\n",
        "#作業フォルダを移動\n",
        "%cd brain-segmentation-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPPewOUcW1Qo",
        "outputId": "376998f1-55e3-4546-ee2c-74fcc4dc5720"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain-segmentation-pytorch'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 97 (delta 6), reused 2 (delta 1), pack-reused 85\u001b[K\n",
            "Unpacking objects: 100% (97/97), done.\n",
            "/content/brain-segmentation-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggle_3M datasetのダウンロード\n",
        "\n",
        "まずKaggleに登録してAPIの使用許可を申請する必要あり。詳細は下記を参考に。\n",
        "\n",
        "https://www.currypurin.com/entry/2018/kaggle-api"
      ],
      "metadata": {
        "id": "zh-_NsecRKXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# kaggle ライブラリのインストール\n",
        "!pip install kaggle\n",
        "\n",
        "# 一時フォルダに .kaggleフォルダを作成\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# MyDrive の kaggle.json　(permissionファイル) を一時フォルダ内の .kaggleフォルダにコピー\n",
        "!cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "\n",
        "# アクセス権限の設定\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# zipファイルのダウンロード\n",
        "!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation\n",
        "#!kaggle competitions download -c rsna-2022-cervical-spine-fracture-detection -p /content/drive/MyDrive/Kaggle\n",
        "# 解凍\n",
        "!unzip ./lgg-mri-segmentation.zip -d ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qItwihSrRCuO",
        "outputId": "a47bdb08-383a-495c-a43a-64b954cd29f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "lgg-mri-segmentation.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ここから"
      ],
      "metadata": {
        "id": "Yd4x2Yg3TKt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Requirementsからモジュールをインストール\n",
        "#※バージョンがconflictしまくるのでバージョン指定なし\n",
        "#medpy以外はすでに入っている\n",
        "\n",
        "modules = \"\"\"\n",
        "numpy\n",
        "tensorflow\n",
        "scikit-learn\n",
        "scikit-image\n",
        "imageio\n",
        "medpy\n",
        "Pillow\n",
        "scipy\n",
        "pandas\n",
        "tqdm\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", mode=\"w\") as f:\n",
        "    f.write(modules)\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from medpy.filter.binary import largest_connected_component\n",
        "from skimage.io import imsave\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import BrainSegmentationDataset as Dataset\n",
        "from unet import UNet\n",
        "from utils import dsc, gray2rgb, outline\n"
      ],
      "metadata": {
        "id": "3Q1C07gvZFBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**interference**"
      ],
      "metadata": {
        "id": "kZRO0btlksID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_per_volume(\n",
        "    input_list, pred_list, true_list, patient_slice_index, patients\n",
        "):\n",
        "    volumes = {}\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
        "        volume_pred = np.round(\n",
        "            np.array(pred_list[index : index + num_slices[p]])\n",
        "        ).astype(int)\n",
        "        volume_pred = largest_connected_component(volume_pred)\n",
        "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
        "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
        "        index += num_slices[p]\n",
        "    return volumes\n",
        "\n",
        "def dsc_distribution(volumes):\n",
        "    dsc_dict = {}\n",
        "    for p in volumes:\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        dsc_dict[p] = dsc(y_pred, y_true, lcc=False)\n",
        "    return dsc_dict\n",
        "\n",
        "def plot_dsc(dsc_dist):\n",
        "    y_positions = np.arange(len(dsc_dist))\n",
        "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
        "    values = [x[1] for x in dsc_dist]\n",
        "    labels = [x[0] for x in dsc_dist]\n",
        "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
        "    plt.yticks(y_positions, labels)\n",
        "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
        "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
        "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
        "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    canvas.draw()\n",
        "    plt.close()\n",
        "    s, (width, height) = canvas.print_to_buffer()\n",
        "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n"
      ],
      "metadata": {
        "id": "Unh_DrpaikGH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "weights_dir = \"./weights/unet.pt\" #weightの保存先\n",
        "#############################\n",
        "\n",
        "#predictionsフォルダ作成\n",
        "os.makedirs(\"./predictions\", exist_ok=True)\n",
        "\n",
        "#deviceを定義\n",
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "\n",
        "#データセットとデータローダー\n",
        "dataset = Dataset(\n",
        "    images_dir=\"./lgg-mri-segmentation/kaggle_3m\", #kaggle dataset使用\n",
        "    subset=\"validation\",\n",
        "    image_size=256,\n",
        "    random_sampling=False,\n",
        ")\n",
        "loader = DataLoader(\n",
        "    dataset, batch_size=32, drop_last=False, num_workers=1\n",
        ")\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "    unet = UNet(in_channels=Dataset.in_channels, out_channels=Dataset.out_channels)\n",
        "    \n",
        "    #モデルのweightをロード\n",
        "    state_dict = torch.load(weights_dir, map_location=device)\n",
        "    unet.load_state_dict(state_dict)\n",
        "\n",
        "    unet.eval()\n",
        "    unet.to(device)\n",
        "\n",
        "    input_list = []\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "\n",
        "    for i, data in tqdm(enumerate(loader)):\n",
        "        x, y_true = data\n",
        "        x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "        y_pred = unet(x)\n",
        "        y_pred_np = y_pred.detach().cpu().numpy()\n",
        "        pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
        "\n",
        "        y_true_np = y_true.detach().cpu().numpy()\n",
        "        true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
        "\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "        input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
        "\n",
        "    volumes = postprocess_per_volume(\n",
        "        input_list,\n",
        "        pred_list,\n",
        "        true_list,\n",
        "        loader.dataset.patient_slice_index,\n",
        "        loader.dataset.patients,\n",
        "    )\n",
        "\n",
        "    dsc_dist = dsc_distribution(volumes)\n",
        "\n",
        "    dsc_dist_plot = plot_dsc(dsc_dist)\n",
        "    imsave(args.figure, dsc_dist_plot)\n",
        "\n",
        "    for p in volumes:\n",
        "        x = volumes[p][0]\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        for s in range(x.shape[0]):\n",
        "            image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
        "            image = outline(image, y_pred[s, 0], color=[255, 0, 0])\n",
        "            image = outline(image, y_true[s, 0], color=[0, 255, 0])\n",
        "            filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
        "            filepath = os.path.join(args.predictions, filename)\n",
        "            imsave(filepath, image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vquaAK63hlr",
        "outputId": "e43d023a-8ad0-469e-8a9e-2c45fb515a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading validation images...\n",
            "preprocessing validation volumes...\n",
            "cropping validation volumes...\n",
            "padding validation volumes...\n",
            "resizing validation volumes...\n"
          ]
        }
      ]
    }
  ]
}