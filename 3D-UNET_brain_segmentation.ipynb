{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled25.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNf8FUJISN+BbqhTDTS/2ZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Article_implementation/blob/main/3D-UNET_brain_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Brain_segmentation_pytorch 3D-UNET**\n",
        "\n",
        "CTの複数画像を立体的にsegmentationする\n",
        "\n",
        "GitHub: hhttps://github.com/mateuszbuda/brain-segmentation-pytorch\n",
        "\n",
        "WebPage: https://towardsdatascience.com/creating-and-training-a-u-net-model-with-pytorch-for-2d-3d-semantic-segmentation-model-building-6ab09d6a0862\n"
      ],
      "metadata": {
        "id": "MTShhTbs_oB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mateuszbuda/brain-segmentation-pytorch.git\n",
        "\n",
        "#作業フォルダを移動\n",
        "%cd brain-segmentation-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPPewOUcW1Qo",
        "outputId": "2f12ec50-207c-40b4-ab59-089a01aebc94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain-segmentation-pytorch'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 97 (delta 6), reused 2 (delta 1), pack-reused 84\u001b[K\n",
            "Unpacking objects: 100% (97/97), done.\n",
            "/content/brain-segmentation-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggle_3M datasetのダウンロード\n",
        "\n",
        "まずKaggleに登録してAPIの使用許可を申請する必要あり。詳細は下記を参考に。\n",
        "\n",
        "https://www.currypurin.com/entry/2018/kaggle-api"
      ],
      "metadata": {
        "id": "zh-_NsecRKXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# kaggle ライブラリのインストール\n",
        "!pip install kaggle\n",
        "\n",
        "# 一時フォルダに .kaggleフォルダを作成\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# MyDrive の kaggle.json　(permissionファイル) を一時フォルダ内の .kaggleフォルダにコピー\n",
        "!cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "\n",
        "# アクセス権限の設定\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# zipファイルのダウンロード\n",
        "!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation\n",
        "#!kaggle competitions download -c rsna-2022-cervical-spine-fracture-detection -p /content/drive/MyDrive/Kaggle\n",
        "# 解凍\n",
        "!unzip ./lgg-mri-segmentation.zip -d ./"
      ],
      "metadata": {
        "id": "qItwihSrRCuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4bf2c97-e508-4b4f-c4bb-4e5e7158a09f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "lgg-mri-segmentation.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  ./lgg-mri-segmentation.zip\n",
            "replace ./kaggle_3m/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ここから"
      ],
      "metadata": {
        "id": "Yd4x2Yg3TKt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Requirementsからモジュールをインストール\n",
        "#※バージョンがconflictしまくるのでバージョン指定なし\n",
        "#medpy以外はすでに入っている\n",
        "\n",
        "modules = \"\"\"\n",
        "numpy\n",
        "tensorflow\n",
        "scikit-learn\n",
        "scikit-image\n",
        "imageio\n",
        "medpy\n",
        "Pillow\n",
        "scipy\n",
        "pandas\n",
        "tqdm\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", mode=\"w\") as f:\n",
        "    f.write(modules)\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from medpy.filter.binary import largest_connected_component\n",
        "from skimage.io import imsave\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import BrainSegmentationDataset as Dataset\n",
        "from unet import UNet\n",
        "from utils import dsc, gray2rgb, outline\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n"
      ],
      "metadata": {
        "id": "3Q1C07gvZFBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modules**"
      ],
      "metadata": {
        "id": "kZRO0btlksID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_per_volume(\n",
        "    input_list, pred_list, true_list, patient_slice_index, patients\n",
        "):\n",
        "    volumes = {}\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index]) #各要素が何個ずつあるかを数える\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
        "        volume_pred = np.round(\n",
        "            np.array(pred_list[index : index + num_slices[p]])\n",
        "        ).astype(int)\n",
        "        volume_pred = largest_connected_component(volume_pred)\n",
        "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
        "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
        "        index += num_slices[p]\n",
        "    return volumes\n",
        "\n",
        "def dsc_distribution(volumes):\n",
        "    dsc_dict = {}\n",
        "    for p in volumes:\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        dsc_dict[p] = dsc(y_pred, y_true, lcc=False)\n",
        "    return dsc_dict\n",
        "\n",
        "def plot_dsc(dsc_dist):\n",
        "    y_positions = np.arange(len(dsc_dist))\n",
        "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
        "    values = [x[1] for x in dsc_dist]\n",
        "    labels = [x[0] for x in dsc_dist]\n",
        "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
        "    plt.yticks(y_positions, labels)\n",
        "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
        "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
        "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
        "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    canvas.draw()\n",
        "    plt.close()\n",
        "    s, (width, height) = canvas.print_to_buffer()\n",
        "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
        "\n",
        "def outline(image, mask, color):\n",
        "    mask = np.round(mask)\n",
        "    yy, xx = np.nonzero(mask)\n",
        "    for y, x in zip(yy, xx):\n",
        "        if 0.0 < np.mean(mask[max(0, y - 1) : y + 2, max(0, x - 1) : x + 2]) < 1.0:\n",
        "            image[max(0, y) : y + 1, max(0, x) : x + 1] = color\n",
        "    return image\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.io import imread\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.transform import resize\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from utils import crop_sample, pad_sample, resize_sample, normalize_volume\n",
        "\n",
        "\n",
        "# #dataset (datasetフォルダに入っているのでここに書く必要ないが、解説のために再定義)\n",
        "class BrainSegmentationDataset(Dataset):\n",
        "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
        "\n",
        "    in_channels = 3\n",
        "    out_channels = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir,\n",
        "        transform=None,\n",
        "        image_size=256,\n",
        "        subset=\"train\",\n",
        "        random_sampling=True,\n",
        "        validation_cases=10,\n",
        "        seed=42,\n",
        "    ):\n",
        "        assert subset in [\"all\", \"train\", \"validation\"] #all, train,validation以外のsubsetにするとエラーを出す\n",
        "\n",
        "        # read images\n",
        "        volumes = {}\n",
        "        masks = {}\n",
        "        print(\"reading {} images...\".format(subset))\n",
        "        for (dirpath, dirnames, filenames) in os.walk(images_dir): #dirpath: 親フォルダのパス、filenames: ファイルの名前\n",
        "            image_slices = []\n",
        "            mask_slices = []\n",
        "\n",
        "            for filename in sorted(\n",
        "                filter(lambda f: \".tif\" in f, filenames),  \n",
        "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
        "            ): #tifがついているファイルを番号順にソートする\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                if \"mask\" in filename: #maskがファイル名についているもの\n",
        "                    mask_slices.append(imread(filepath, as_gray=True))\n",
        "                else: #ついていないもの\n",
        "                    image_slices.append(imread(filepath))\n",
        "            if len(image_slices) > 0:\n",
        "                patient_id = dirpath.split(\"/\")[-1] #TCGA_HT_8018_19970411\n",
        "                volumes[patient_id] = np.array(image_slices[1:-1]) #volumes: マスクなし画像,RGB(最初と最後の1枚ずつを除外する（informationなどがある？？？？）)\n",
        "                masks[patient_id] = np.array(mask_slices[1:-1]) #masks: マスク画像, grayscale\n",
        "\n",
        "        print(len(volumes))\n",
        "        print(len(masks))\n",
        "\n",
        "        self.patients = sorted(volumes) #patientsのリストをソート\n",
        "\n",
        "        # select cases to subset\n",
        "        if not subset == \"all\":\n",
        "            random.seed(seed)\n",
        "            validation_patients = random.sample(self.patients, k=validation_cases) #validation_casesで指定した分だけ無作為に抜き出す\n",
        "            if subset == \"validation\":\n",
        "                self.patients = validation_patients\n",
        "            else:\n",
        "                self.patients = sorted(\n",
        "                    list(set(self.patients).difference(validation_patients))\n",
        "                )\n",
        "\n",
        "        print(\"preprocessing {} volumes...\".format(subset))\n",
        "        \n",
        "        # create list of tuples (volume, mask)\n",
        "        self.volumes = [(volumes[k], masks[k]) for k in self.patients] #スライスの数だけvolumesとmaskのペアを作る\n",
        "\n",
        "\n",
        "        ##############################################################\n",
        "        # print(f\"volumes: {volumes[self.patients[2]].shape}\") #(18,256,256,3) --> 枚数、縦、横、RGB\n",
        "        # print(f\"masks: {masks[self.patients[2]].shape}\") #(18,256,256)\n",
        "        ##############################################################\n",
        "        ##################################################################\n",
        "        print(f\"normalize_volume: {self.volumes[2][0].shape}\") #2症例目の0(volume)の形状 (18,256,256,3) --> 枚数、縦、横、RGB\n",
        "        print(f\"normalize_masks: {self.volumes[2][1].shape}\") #2症例目の1(masks)の形状 (18,256,256)\n",
        "        #################################################################\n",
        "\n",
        "        print(\"cropping {} volumes...\".format(subset))\n",
        "        # crop to smallest enclosing volume ...何をしているのかいまいちよくわからない...\n",
        "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
        "\n",
        "\n",
        "        print(\"padding {} volumes...\".format(subset))\n",
        "        # pad to square  横長の画像を正方形に\n",
        "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
        "\n",
        "        print(\"resizing {} volumes...\".format(subset))\n",
        "        # resize #256*256にリサイズ\n",
        "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
        "\n",
        "        print(\"normalizing {} volumes...\".format(subset))\n",
        "        # normalize channel-wise\n",
        "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]  #v: volume, m: mask、上下の10%を除去してノーマライズ\n",
        "\n",
        "        ##################################################################\n",
        "        print(f\"normalized_volume_shape: {self.volumes[0][0].shape}\") #0症例目の0(volume)の形状(18,256,256,3)\n",
        "        print(f\"normalized_masks_shape: {self.volumes[0][1].shape}\") #0症例目の1(masks)の形状(18,256,256)\n",
        "        #################################################################\n",
        "\n",
        "\n",
        "        # probabilities for sampling slices based on masks\n",
        "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes] #(10)\n",
        "        print(len(self.slice_weights))\n",
        "        self.slice_weights = [\n",
        "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights #(10)\n",
        "        ]\n",
        "        print(f\"n_slice_weights: {len(self.slice_weights)}\")\n",
        "\n",
        "\n",
        "        # add channel dimension to masks\n",
        "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes] \n",
        "        ##################################################################\n",
        "        print(f\"final_volume_shape: {self.volumes[0][0].shape}\") #0症例目の0(volume)の形状 (18,256,256,3) --> 枚数、縦、横、RGB\n",
        "        print(f\"final_masks_shape: {self.volumes[0][1].shape}\") #2症例目の1(masks)の形状 (18,256,256,1) <--最後に1の次元を追加\n",
        "        #################################################################\n",
        "        #################################\n",
        "        print(len(self.volumes)) #10\n",
        "        #################################\n",
        "\n",
        "        print(\"done creating {} dataset\".format(subset))\n",
        "\n",
        "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
        "        # [(0,0), (0,1),(0,2)...(0,18), (1,0), (1,1), (1,3), ...(1,18)...] \n",
        "        num_slices = [v.shape[0] for v, m in self.volumes] #スライス数\n",
        "        self.patient_slice_index = list(\n",
        "            zip(\n",
        "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []), \n",
        "                sum([list(range(x)) for x in num_slices], []), \n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.random_sampling = random_sampling\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_slice_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient = self.patient_slice_index[idx][0]\n",
        "        slice_n = self.patient_slice_index[idx][1]\n",
        "        #print(f\"idx:{idx}, patient:{patient}, slice_n:{slice_n}\")\n",
        "\n",
        "        if self.random_sampling:\n",
        "            patient = np.random.randint(len(self.volumes))\n",
        "            slice_n = np.random.choice(\n",
        "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
        "            )\n",
        " \n",
        "        v, m = self.volumes[patient]\n",
        "        image = v[slice_n] #(256,256,3)\n",
        "        mask = m[slice_n] #(256,256,1)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform((image, mask))\n",
        "\n",
        "        # fix dimensions (C, H, W)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        mask = mask.transpose(2, 0, 1)\n",
        "\n",
        "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
        "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
        "\n",
        "        # return tensors\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "\n",
        "#以下utilisより抜粋（utilisよりimportされているのでここで定義しなくてもOK）\n",
        "def crop_sample(x):\n",
        "    volume, mask = x\n",
        "    volume[volume < np.max(volume) * 0.1] = 0 #最高densityの0.1倍未満のものはゼロに切り捨てる\n",
        "    z_projection = np.max(np.max(np.max(volume, axis=-1), axis=-1), axis=-1)\n",
        "    z_nonzero = np.nonzero(z_projection)\n",
        "    z_min = np.min(z_nonzero)\n",
        "    z_max = np.max(z_nonzero) + 1\n",
        "    y_projection = np.max(np.max(np.max(volume, axis=0), axis=-1), axis=-1)\n",
        "    y_nonzero = np.nonzero(y_projection)\n",
        "    y_min = np.min(y_nonzero)\n",
        "    y_max = np.max(y_nonzero) + 1\n",
        "    x_projection = np.max(np.max(np.max(volume, axis=0), axis=0), axis=-1)\n",
        "    x_nonzero = np.nonzero(x_projection)\n",
        "    x_min = np.min(x_nonzero)\n",
        "    x_max = np.max(x_nonzero) + 1\n",
        "    return (\n",
        "        volume[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "        mask[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "    )\n",
        "\n",
        "def pad_sample(x): #横長の画像を正方形にする\n",
        "    volume, mask = x\n",
        "    a = volume.shape[1]\n",
        "    b = volume.shape[2]\n",
        "    if a == b:\n",
        "        return volume, mask\n",
        "    diff = (max(a, b) - min(a, b)) / 2.0\n",
        "    if a > b:\n",
        "        padding = ((0, 0), (0, 0), (int(np.floor(diff)), int(np.ceil(diff))))\n",
        "    else:\n",
        "        padding = ((0, 0), (int(np.floor(diff)), int(np.ceil(diff))), (0, 0))\n",
        "    mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
        "    padding = padding + ((0, 0),)\n",
        "    volume = np.pad(volume, padding, mode=\"constant\", constant_values=0)\n",
        "    return volume, mask\n",
        "\n",
        "def normalize_volume(volume):\n",
        "    p10 = np.percentile(volume, 10)\n",
        "    p99 = np.percentile(volume, 99)\n",
        "    volume = rescale_intensity(volume, in_range=(p10, p99)) #skimageを用いてintensityの上下を切ってnormalizeする\n",
        "    m = np.mean(volume, axis=(0, 1, 2))\n",
        "    s = np.std(volume, axis=(0, 1, 2))\n",
        "    volume = (volume - m) / s\n",
        "    return volume\n",
        "\n",
        "def resize_sample(x, size=256): #skimage.transform.resizeを用いてsize=256にリサイズ）\n",
        "    volume, mask = x\n",
        "    v_shape = volume.shape\n",
        "    out_shape = (v_shape[0], size, size)\n",
        "    mask = resize(\n",
        "        mask,\n",
        "        output_shape=out_shape,\n",
        "        order=0,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    ) #order=0: nearest neighbor\n",
        "    out_shape = out_shape + (v_shape[3],)\n",
        "    volume = resize(\n",
        "        volume,\n",
        "        output_shape=out_shape,\n",
        "        order=2,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    ) #order=2: bi-quadratic\n",
        "    return volume, mask\n"
      ],
      "metadata": {
        "id": "Unh_DrpaikGH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference**"
      ],
      "metadata": {
        "id": "hg7C1RlML3R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#これをインポートすれば上のmoduleは不要\n",
        "#from dataset import BrainSegmentationDataset as Dataset\n",
        "\n",
        "\n",
        "#############################\n",
        "#weights_dir = \"./weights/unet.pt\" #weightの保存先\n",
        "weights_dir = \"/content/drive/MyDrive/Kaggle/Brain_segmentation_3dUNET/unet.pt\" #下でtrainingしたモデルを用いる場合\n",
        "#############################\n",
        "\n",
        "#predictionsフォルダ作成\n",
        "os.makedirs(\"./predictions\", exist_ok=True)\n",
        "\n",
        "#deviceを定義\n",
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "\n",
        "#データセットとデータローダー\n",
        "dataset = BrainSegmentationDataset(\n",
        "    images_dir=\"/content/brain-segmentation-pytorch/kaggle_3m\", #kaggle dataset使用\n",
        "    subset=\"validation\",\n",
        "    image_size=256,\n",
        "    random_sampling=False,\n",
        ")\n",
        "loader = DataLoader(\n",
        "    dataset, batch_size=32, drop_last=False, num_workers=0\n",
        ")\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "    unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "    \n",
        "    #モデルのweightをロード\n",
        "    state_dict = torch.load(weights_dir, map_location=device)\n",
        "    unet.load_state_dict(state_dict)\n",
        "\n",
        "    unet.eval()\n",
        "    unet.to(device)\n",
        "\n",
        "    input_list = []\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "\n",
        "    for i, data in tqdm(enumerate(loader)):\n",
        "        x, y_true = data\n",
        "        x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "        y_pred = unet(x)\n",
        "        y_pred_np = y_pred.detach().cpu().numpy()\n",
        "        pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
        "\n",
        "        y_true_np = y_true.detach().cpu().numpy()\n",
        "        true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
        "\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "        input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
        "\n",
        "    volumes = postprocess_per_volume(\n",
        "        input_list,\n",
        "        pred_list,\n",
        "        true_list,\n",
        "        loader.dataset.patient_slice_index,\n",
        "        loader.dataset.patients,\n",
        "    )\n",
        "\n",
        "    dsc_dist = dsc_distribution(volumes)\n",
        "\n",
        "    dsc_dist_plot = plot_dsc(dsc_dist)\n",
        "    imsave(\"./dsc.png\", dsc_dist_plot)\n",
        "\n",
        "    for p in volumes:\n",
        "        x = volumes[p][0]\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        for s in range(x.shape[0]):\n",
        "            image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
        "            image = outline(image, y_pred[s, 0], color=[255, 0, 0]) #赤: prediction\n",
        "            image = outline(image, y_true[s, 0], color=[0, 255, 0]) #緑：groundtruth\n",
        "            filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
        "            filepath = os.path.join(\"./predictions\", filename)\n",
        "            imsave(filepath, image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vquaAK63hlr",
        "outputId": "9ed3f93c-3f37-4008-bef5-5ecd86279dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading validation images...\n",
            "110\n",
            "110\n",
            "preprocessing validation volumes...\n",
            "normalize_volume: (18, 256, 256, 3)\n",
            "normalize_masks: (18, 256, 256)\n",
            "cropping validation volumes...\n",
            "padding validation volumes...\n",
            "resizing validation volumes...\n",
            "normalizing validation volumes...\n",
            "normalized_volume_shape: (28, 256, 256, 3)\n",
            "normalized_masks_shape: (28, 256, 256)\n",
            "10\n",
            "n_slice_weights: 10\n",
            "final_volume_shape: (28, 256, 256, 3)\n",
            "final_masks_shape: (28, 256, 256, 1)\n",
            "10\n",
            "done creating validation dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11it [00:08,  1.27it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: ./predictions/TCGA_DU_7014_19860618-57.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: ./predictions/TCGA_DU_6408_19860521-51.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: ./predictions/TCGA_DU_6408_19860521-52.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: ./predictions/TCGA_DU_6408_19860521-53.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: ./predictions/TCGA_DU_6404_19850629-50.png is a low contrast image\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: ./predictions/TCGA_DU_5851_19950428-34.png is a low contrast image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prediction結果を表示 ###\n",
        "\n",
        "3列で全てを表示"
      ],
      "metadata": {
        "id": "EIKSAedanJ3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction結果を表示 (赤：pred、緑：groundtruth)\n",
        "images = [Image.open(img) for img in glob.glob(\"./predictions/*\")[0:90]]\n",
        "\n",
        "cols =3\n",
        "rows = len(images)//cols+1 #縦の行\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(cols*5, rows*5))\n",
        "\n",
        "\n",
        "for i, im in enumerate(images):\n",
        "    fig.add_subplot(rows, cols, i+1).set_title(str(i+1))\n",
        "    plt.imshow(im)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELO1o_fybS4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train**\n",
        "\n",
        "※loggerの部分はtensorflow1→tensorflow2に書き直さないといけないので、省略しています"
      ],
      "metadata": {
        "id": "7SqUW3wHzdW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from dataset import BrainSegmentationDataset as Dataset #上で定義し直しているのでインポートしない\n",
        "#from logger import Logger\n",
        "from loss import DiceLoss\n",
        "from transform import transforms\n",
        "from unet import UNet\n",
        "from utils import log_images, dsc\n",
        "\n",
        "from statistics import mean\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
        "    dsc_list = []\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index]) #要素の数を数える、すなわち患者毎の枚数をリスト化する\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
        "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
        "        dsc_list.append(dsc(y_pred, y_true))\n",
        "        index += num_slices[p]\n",
        "    return dsc_list\n",
        "\n",
        "\n",
        "def log_loss_summary(logger, loss, step, prefix=\"\"):\n",
        "    logger.scalar_summary(prefix + \"loss\", np.mean(loss), step)\n",
        "\n",
        "\n",
        "\n",
        "#main\n",
        "\n",
        "#時間測定開始\n",
        "time_start = time.perf_counter()\n",
        "\n",
        "#random_seed\n",
        "np.random.seed = 42\n",
        "\n",
        "#parameters\n",
        "batch_size = 8\n",
        "image_size = 256\n",
        "num_workers = 0\n",
        "lr = 0.0001\n",
        "n_epochs = 100\n",
        "vis_freq = 10 #frequency of saving images to log file\n",
        "vis_images = 200 #number of visualization images to save in log file\n",
        "#weight_path = \"./weights\"\n",
        "weights_dir = \"/content/drive/MyDrive/Kaggle/Brain_segmentation_3dUNET\"\n",
        "load_weight = False #Gdriveに保存しているweightをロードするかどうか\n",
        "earlystopping = 10 #０にするとearlystoppingをoffにする\n",
        "\n",
        "#フォルダ作成\n",
        "os.makedirs(\"./weights\", exist_ok=True)\n",
        "os.makedirs(\"./logs\", exist_ok=True)\n",
        "\n",
        "# snapshotargs()\n",
        "\n",
        "#deviceを定義\n",
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "\n",
        "\n",
        "if 'loader_valid' in globals(): #データローダー作成済みなら省略する（読み込みに時間かかるので）\n",
        "    pass\n",
        "else:\n",
        "    #dataset\n",
        "    dataset_train = BrainSegmentationDataset(\n",
        "        images_dir=\"/content/brain-segmentation-pytorch/kaggle_3m\",\n",
        "        subset=\"train\",\n",
        "        image_size=image_size,\n",
        "        transform=transforms(scale=0.05, angle=15, flip_prob=0.5), #scale, angle: augmentationの拡大縮小および回転角度\n",
        "    )\n",
        "    dataset_valid = BrainSegmentationDataset(\n",
        "        images_dir=\"/content/brain-segmentation-pytorch/kaggle_3m\",\n",
        "        subset=\"validation\",\n",
        "        image_size=image_size,\n",
        "        random_sampling=False,\n",
        "    )\n",
        "\n",
        "    #dataloader\n",
        "    loader_train = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=num_workers,\n",
        "        worker_init_fn= None,\n",
        "    )\n",
        "    loader_valid = DataLoader(\n",
        "        dataset_valid,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers,\n",
        "        worker_init_fn= None,\n",
        "    )\n",
        "    loaders = {\"train\": loader_train, \"valid\": loader_valid}\n",
        "\n",
        "    print(f\"elapsed_time: {time.perf_counter() - time_start}\")\n",
        "    print(\"\")\n",
        "\n",
        "#model\n",
        "unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "unet.to(device)\n",
        "\n",
        "if load_weight is True:\n",
        "    unet.load_state_dict(torch.load (os.path.join(weights_dir, \"unet.pt\")))\n",
        "    print(\"loading weight...\")\n",
        "else:\n",
        "    pass\n",
        "\n",
        "dsc_loss = DiceLoss()\n",
        "best_validation_dsc = 0.0\n",
        "\n",
        "optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
        "\n",
        "#logger = Logger(\"./logs\")\n",
        "loss_train = []\n",
        "loss_valid = []\n",
        "\n",
        "step = 0\n",
        "earlystopping_counter = 0\n",
        "time_start = time.perf_counter() #時間測定開始\n",
        "for epoch in tqdm(range(n_epochs), total=n_epochs): \n",
        "    for phase in [\"train\", \"valid\"]:\n",
        "        if phase == \"train\":\n",
        "            unet.train()\n",
        "        else:\n",
        "            unet.eval()\n",
        "\n",
        "        validation_pred = []\n",
        "        validation_true = []\n",
        "\n",
        "        for i, data in enumerate(loaders[phase]):\n",
        "            if phase == \"train\":\n",
        "                step += 1\n",
        "\n",
        "            x, y_true = data #x: 入力画像、y_true: ラベル画像\n",
        "            x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                y_pred = unet(x)\n",
        "\n",
        "                loss = dsc_loss(y_pred, y_true)\n",
        "\n",
        "                if phase == \"valid\":\n",
        "                    loss_valid.append(loss.item())\n",
        "                    val_loss = loss.item() #途中経過表示用\n",
        "                    y_pred_np = y_pred.detach().cpu().numpy()\n",
        "                    validation_pred.extend(\n",
        "                        [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
        "                    )\n",
        "                    y_true_np = y_true.detach().cpu().numpy()\n",
        "                    validation_true.extend(\n",
        "                        [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
        "                    )\n",
        "                    if (epoch % vis_freq == 0) or (epoch == n_epochs - 1):\n",
        "                        if i * batch_size < vis_images:\n",
        "                            tag = \"image/{}\".format(i)\n",
        "                            num_images = vis_images - i * batch_size\n",
        "                            # logger.image_list_summary(\n",
        "                            #     tag,\n",
        "                            #     log_images(x, y_true, y_pred)[:num_images],\n",
        "                            #     step,\n",
        "                            # )\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    loss_train.append(loss.item())\n",
        "                    train_loss = loss.item() #途中経過表示用\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            if phase == \"train\" and (step + 1) % 10 == 0:\n",
        "                #log_loss_summary(logger, loss_train, step)\n",
        "                loss_train = []\n",
        "\n",
        "        if phase == \"valid\":\n",
        "            #log_loss_summary(logger, loss_valid, step, prefix=\"val_\")\n",
        "            mean_dsc = np.mean(\n",
        "                dsc_per_volume(\n",
        "                    validation_pred,\n",
        "                    validation_true,\n",
        "                    loader_valid.dataset.patient_slice_index,\n",
        "                )\n",
        "            )\n",
        "            # logger.scalar_summary(\"val_dsc\", mean_dsc, step)\n",
        "            \n",
        "            ####途中経過####\n",
        "            print(\"\")\n",
        "            print(f\"epoch: {str(epoch+1)}\")\n",
        "            print(f\"train_loss: {train_loss:.5f}\")\n",
        "            print(f\"val_loss: {val_loss:.5f}\")\n",
        "            print(f\"val_dsc: {mean_dsc:.5f}\") \n",
        "            print(f\"elapsed_time: {time.perf_counter() - time_start:.5f}\")\n",
        "            \n",
        "            if mean_dsc > best_validation_dsc:\n",
        "                print(f'mean_dsc increased ({best_validation_dsc:5f} --> {mean_dsc:5f}). Saving model...')\n",
        "                best_validation_dsc = mean_dsc\n",
        "                torch.save(unet.state_dict(), os.path.join(weights_dir, \"unet.pt\"))\n",
        "                earlystopping_counter = 0 #reset earlystopping\n",
        "            else:\n",
        "                earlystopping_counter += 1\n",
        "                if earlystopping >= 1:\n",
        "                    print(f\"earlystopping_counter: {earlystopping_counter}\")\n",
        "                    if earlystopping_counter == earlystopping:\n",
        "                          print(\"The training stopped with earlystopping!\")\n",
        "                          break\n",
        "            print(\"\")\n",
        "            loss_valid = []\n",
        "\n",
        "print(\"Best validation mean DSC: {:4f}\".format(best_validation_dsc))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7jR9teGPzfiY",
        "outputId": "ca664a47-dd39-4f1d-8d38-d8426c614d6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [01:36<2:39:50, 96.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 1\n",
            "train_loss: 0.78171\n",
            "val_loss: 0.90799\n",
            "val_dsc: 0.59761\n",
            "elapsed_time: 96.75017\n",
            "mean_dsc increased (0.000000 --> 0.597606). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [03:12<2:37:05, 96.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 2\n",
            "train_loss: 0.78682\n",
            "val_loss: 0.87244\n",
            "val_dsc: 0.71170\n",
            "elapsed_time: 192.45216\n",
            "mean_dsc increased (0.597606 --> 0.711697). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [04:47<2:34:27, 95.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 3\n",
            "train_loss: 0.47919\n",
            "val_loss: 0.80471\n",
            "val_dsc: 0.77219\n",
            "elapsed_time: 287.22216\n",
            "mean_dsc increased (0.711697 --> 0.772185). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [06:22<2:32:41, 95.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 4\n",
            "train_loss: 0.45336\n",
            "val_loss: 0.68390\n",
            "val_dsc: 0.78878\n",
            "elapsed_time: 382.49654\n",
            "mean_dsc increased (0.772185 --> 0.788783). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [07:59<2:31:41, 95.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 5\n",
            "train_loss: 0.26241\n",
            "val_loss: 0.52789\n",
            "val_dsc: 0.80667\n",
            "elapsed_time: 478.97162\n",
            "mean_dsc increased (0.788783 --> 0.806667). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [08:45<2:46:24, 105.10s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c04944817709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#途中経過表示用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}