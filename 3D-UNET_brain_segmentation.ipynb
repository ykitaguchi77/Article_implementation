{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled25.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN6f6wbM8yNFTkJvIvA4wky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Article_implementation/blob/main/3D-UNET_brain_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Brain_segmentation_pytorch 3D-UNET**\n",
        "\n",
        "CTの複数画像を立体的にsegmentationする\n",
        "\n",
        "GitHub: hhttps://github.com/mateuszbuda/brain-segmentation-pytorch\n",
        "\n",
        "WebPage: https://towardsdatascience.com/creating-and-training-a-u-net-model-with-pytorch-for-2d-3d-semantic-segmentation-model-building-6ab09d6a0862\n"
      ],
      "metadata": {
        "id": "MTShhTbs_oB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mateuszbuda/brain-segmentation-pytorch.git\n",
        "\n",
        "#作業フォルダを移動\n",
        "%cd brain-segmentation-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPPewOUcW1Qo",
        "outputId": "dc3c1895-f8af-4bb2-dbf0-6e21d6f1c2a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain-segmentation-pytorch'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 97 (delta 6), reused 2 (delta 1), pack-reused 85\u001b[K\n",
            "Unpacking objects: 100% (97/97), done.\n",
            "/content/brain-segmentation-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kaggle_3M datasetのダウンロード\n",
        "\n",
        "まずKaggleに登録してAPIの使用許可を申請する必要あり。詳細は下記を参考に。\n",
        "\n",
        "https://www.currypurin.com/entry/2018/kaggle-api"
      ],
      "metadata": {
        "id": "zh-_NsecRKXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# kaggle ライブラリのインストール\n",
        "!pip install kaggle\n",
        "\n",
        "# 一時フォルダに .kaggleフォルダを作成\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# MyDrive の kaggle.json　(permissionファイル) を一時フォルダ内の .kaggleフォルダにコピー\n",
        "!cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "\n",
        "# アクセス権限の設定\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# zipファイルのダウンロード\n",
        "!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation\n",
        "#!kaggle competitions download -c rsna-2022-cervical-spine-fracture-detection -p /content/drive/MyDrive/Kaggle\n",
        "# 解凍\n",
        "!unzip ./lgg-mri-segmentation.zip -d ./"
      ],
      "metadata": {
        "id": "qItwihSrRCuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ここから"
      ],
      "metadata": {
        "id": "Yd4x2Yg3TKt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Requirementsからモジュールをインストール\n",
        "#※バージョンがconflictしまくるのでバージョン指定なし\n",
        "#medpy以外はすでに入っている\n",
        "\n",
        "modules = \"\"\"\n",
        "numpy\n",
        "tensorflow\n",
        "scikit-learn\n",
        "scikit-image\n",
        "imageio\n",
        "medpy\n",
        "Pillow\n",
        "scipy\n",
        "pandas\n",
        "tqdm\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", mode=\"w\") as f:\n",
        "    f.write(modules)\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from medpy.filter.binary import largest_connected_component\n",
        "from skimage.io import imsave\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import BrainSegmentationDataset as Dataset\n",
        "from unet import UNet\n",
        "from utils import dsc, gray2rgb, outline\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n"
      ],
      "metadata": {
        "id": "3Q1C07gvZFBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modules**"
      ],
      "metadata": {
        "id": "kZRO0btlksID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_per_volume(\n",
        "    input_list, pred_list, true_list, patient_slice_index, patients\n",
        "):\n",
        "    volumes = {}\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index]) #各要素が何個ずつあるかを数える\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
        "        volume_pred = np.round(\n",
        "            np.array(pred_list[index : index + num_slices[p]])\n",
        "        ).astype(int)\n",
        "        volume_pred = largest_connected_component(volume_pred)\n",
        "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
        "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
        "        index += num_slices[p]\n",
        "    return volumes\n",
        "\n",
        "def dsc_distribution(volumes):\n",
        "    dsc_dict = {}\n",
        "    for p in volumes:\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        dsc_dict[p] = dsc(y_pred, y_true, lcc=False)\n",
        "    return dsc_dict\n",
        "\n",
        "def plot_dsc(dsc_dist):\n",
        "    y_positions = np.arange(len(dsc_dist))\n",
        "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
        "    values = [x[1] for x in dsc_dist]\n",
        "    labels = [x[0] for x in dsc_dist]\n",
        "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
        "    plt.yticks(y_positions, labels)\n",
        "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
        "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
        "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
        "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    canvas.draw()\n",
        "    plt.close()\n",
        "    s, (width, height) = canvas.print_to_buffer()\n",
        "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
        "\n",
        "def outline(image, mask, color):\n",
        "    mask = np.round(mask)\n",
        "    yy, xx = np.nonzero(mask)\n",
        "    for y, x in zip(yy, xx):\n",
        "        if 0.0 < np.mean(mask[max(0, y - 1) : y + 2, max(0, x - 1) : x + 2]) < 1.0:\n",
        "            image[max(0, y) : y + 1, max(0, x) : x + 1] = color\n",
        "    return image\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.io import imread\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.transform import resize\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from utils import crop_sample, pad_sample, resize_sample, normalize_volume\n",
        "\n",
        "\n",
        "# #dataset (datasetフォルダに入っているのでここに書く必要ないが、解説のために再定義)\n",
        "class BrainSegmentationDataset(Dataset):\n",
        "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
        "\n",
        "    in_channels = 3\n",
        "    out_channels = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir,\n",
        "        transform=None,\n",
        "        image_size=256,\n",
        "        subset=\"train\",\n",
        "        random_sampling=True,\n",
        "        validation_cases=10,\n",
        "        seed=42,\n",
        "    ):\n",
        "        assert subset in [\"all\", \"train\", \"validation\"] #all, train,validation以外のsubsetにするとエラーを出す\n",
        "\n",
        "        # read images\n",
        "        volumes = {}\n",
        "        masks = {}\n",
        "        print(\"reading {} images...\".format(subset))\n",
        "        for (dirpath, dirnames, filenames) in os.walk(images_dir): #dirpath: 親フォルダのパス、filenames: ファイルの名前\n",
        "            image_slices = []\n",
        "            mask_slices = []\n",
        "\n",
        "            for filename in sorted(\n",
        "                filter(lambda f: \".tif\" in f, filenames),  \n",
        "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
        "            ): #tifがついているファイルを番号順にソートする\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                if \"mask\" in filename: #maskがファイル名についているもの\n",
        "                    mask_slices.append(imread(filepath, as_gray=True))\n",
        "                else: #ついていないもの\n",
        "                    image_slices.append(imread(filepath))\n",
        "            if len(image_slices) > 0:\n",
        "                patient_id = dirpath.split(\"/\")[-1] #TCGA_HT_8018_19970411\n",
        "                volumes[patient_id] = np.array(image_slices[1:-1]) #volumes: マスクなし画像,RGB(最初と最後の1枚ずつを除外する（informationなどがある？？？？）)\n",
        "                masks[patient_id] = np.array(mask_slices[1:-1]) #masks: マスク画像, grayscale\n",
        "\n",
        "        self.patients = sorted(volumes) #patientsのリストをソート\n",
        "\n",
        "        # select cases to subset\n",
        "        if not subset == \"all\":\n",
        "            random.seed(seed)\n",
        "            validation_patients = random.sample(self.patients, k=validation_cases) #validation_casesで指定した分だけ無作為に抜き出す\n",
        "            if subset == \"validation\":\n",
        "                self.patients = validation_patients\n",
        "            else:\n",
        "                self.patients = sorted(\n",
        "                    list(set(self.patients).difference(validation_patients))\n",
        "                )\n",
        "\n",
        "        print(\"preprocessing {} volumes...\".format(subset))\n",
        "        \n",
        "        # create list of tuples (volume, mask)\n",
        "        self.volumes = [(volumes[k], masks[k]) for k in self.patients] #スライスの数だけvolumesとmaskのペアを作る\n",
        "\n",
        "\n",
        "        ##############################################################\n",
        "        # print(f\"volumes: {volumes[self.patients[2]].shape}\") #(18,256,256,3) --> 枚数、縦、横、RGB\n",
        "        # print(f\"masks: {masks[self.patients[2]].shape}\") #(18,256,256)\n",
        "        ##############################################################\n",
        "        ##################################################################\n",
        "        print(f\"normalize_volume: {self.volumes[2][0].shape}\") #2症例目の0(volume)の形状 (18,256,256,3) --> 枚数、縦、横、RGB\n",
        "        print(f\"normalize_masks: {self.volumes[2][1].shape}\") #2症例目の1(masks)の形状 (18,256,256)\n",
        "        #################################################################\n",
        "\n",
        "        print(\"cropping {} volumes...\".format(subset))\n",
        "        # crop to smallest enclosing volume ...何をしているのかいまいちよくわからない...\n",
        "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
        "\n",
        "\n",
        "        print(\"padding {} volumes...\".format(subset))\n",
        "        # pad to square  横長の画像を正方形に\n",
        "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
        "\n",
        "        print(\"resizing {} volumes...\".format(subset))\n",
        "        # resize #256*256にリサイズ\n",
        "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
        "\n",
        "        print(\"normalizing {} volumes...\".format(subset))\n",
        "        # normalize channel-wise\n",
        "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]  #v: volume, m: mask、上下の10%を除去してノーマライズ\n",
        "\n",
        "        ##################################################################\n",
        "        print(f\"normalized_volume_shape: {self.volumes[0][0].shape}\") #0症例目の0(volume)の形状(18,256,256,3)\n",
        "        print(f\"normalized_masks_shape: {self.volumes[0][1].shape}\") #0症例目の1(masks)の形状(18,256,256)\n",
        "        #################################################################\n",
        "\n",
        "\n",
        "        # probabilities for sampling slices based on masks\n",
        "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes] #(10)\n",
        "        print(len(self.slice_weights))\n",
        "        self.slice_weights = [\n",
        "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights #(10)\n",
        "        ]\n",
        "        print(f\"n_slice_weights: {len(self.slice_weights)}\")\n",
        "\n",
        "\n",
        "        # add channel dimension to masks\n",
        "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes] \n",
        "        ##################################################################\n",
        "        print(f\"final_volume_shape: {self.volumes[0][0].shape}\") #0症例目の0(volume)の形状 (18,256,256,3) --> 枚数、縦、横、RGB\n",
        "        print(f\"final_masks_shape: {self.volumes[0][1].shape}\") #2症例目の1(masks)の形状 (18,256,256,1) <--最後に1の次元を追加\n",
        "        #################################################################\n",
        "        #################################\n",
        "        print(len(self.volumes)) #10\n",
        "        #################################\n",
        "\n",
        "        print(\"done creating {} dataset\".format(subset))\n",
        "\n",
        "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
        "        # [(0,0), (0,1),(0,2)...(0,18), (1,0), (1,1), (1,3), ...(1,18)...] \n",
        "        num_slices = [v.shape[0] for v, m in self.volumes] #スライス数\n",
        "        self.patient_slice_index = list(\n",
        "            zip(\n",
        "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []), \n",
        "                sum([list(range(x)) for x in num_slices], []), \n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.random_sampling = random_sampling\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_slice_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient = self.patient_slice_index[idx][0]\n",
        "        slice_n = self.patient_slice_index[idx][1]\n",
        "        #print(f\"idx:{idx}, patient:{patient}, slice_n:{slice_n}\")\n",
        "\n",
        "        if self.random_sampling:\n",
        "            patient = np.random.randint(len(self.volumes))\n",
        "            slice_n = np.random.choice(\n",
        "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
        "            )\n",
        " \n",
        "        v, m = self.volumes[patient]\n",
        "        image = v[slice_n] #(256,256,3)\n",
        "        mask = m[slice_n] #(256,256,1)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform((image, mask))\n",
        "\n",
        "        # fix dimensions (C, H, W)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        mask = mask.transpose(2, 0, 1)\n",
        "\n",
        "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
        "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
        "\n",
        "        # return tensors\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "\n",
        "#以下utilisより抜粋（utilisよりimportされているのでここで定義しなくてもOK）\n",
        "def crop_sample(x):\n",
        "    volume, mask = x\n",
        "    volume[volume < np.max(volume) * 0.1] = 0 #最高densityの0.1倍未満のものはゼロに切り捨てる\n",
        "    z_projection = np.max(np.max(np.max(volume, axis=-1), axis=-1), axis=-1)\n",
        "    z_nonzero = np.nonzero(z_projection)\n",
        "    z_min = np.min(z_nonzero)\n",
        "    z_max = np.max(z_nonzero) + 1\n",
        "    y_projection = np.max(np.max(np.max(volume, axis=0), axis=-1), axis=-1)\n",
        "    y_nonzero = np.nonzero(y_projection)\n",
        "    y_min = np.min(y_nonzero)\n",
        "    y_max = np.max(y_nonzero) + 1\n",
        "    x_projection = np.max(np.max(np.max(volume, axis=0), axis=0), axis=-1)\n",
        "    x_nonzero = np.nonzero(x_projection)\n",
        "    x_min = np.min(x_nonzero)\n",
        "    x_max = np.max(x_nonzero) + 1\n",
        "    return (\n",
        "        volume[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "        mask[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "    )\n",
        "\n",
        "def pad_sample(x): #横長の画像を正方形にする\n",
        "    volume, mask = x\n",
        "    a = volume.shape[1]\n",
        "    b = volume.shape[2]\n",
        "    if a == b:\n",
        "        return volume, mask\n",
        "    diff = (max(a, b) - min(a, b)) / 2.0\n",
        "    if a > b:\n",
        "        padding = ((0, 0), (0, 0), (int(np.floor(diff)), int(np.ceil(diff))))\n",
        "    else:\n",
        "        padding = ((0, 0), (int(np.floor(diff)), int(np.ceil(diff))), (0, 0))\n",
        "    mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
        "    padding = padding + ((0, 0),)\n",
        "    volume = np.pad(volume, padding, mode=\"constant\", constant_values=0)\n",
        "    return volume, mask\n",
        "\n",
        "def normalize_volume(volume):\n",
        "    p10 = np.percentile(volume, 10)\n",
        "    p99 = np.percentile(volume, 99)\n",
        "    volume = rescale_intensity(volume, in_range=(p10, p99)) #skimageを用いてintensityの上下を切ってnormalizeする\n",
        "    m = np.mean(volume, axis=(0, 1, 2))\n",
        "    s = np.std(volume, axis=(0, 1, 2))\n",
        "    volume = (volume - m) / s\n",
        "    return volume\n",
        "\n",
        "def resize_sample(x, size=256): #skimage.transform.resizeを用いてsize=256にリサイズ）\n",
        "    volume, mask = x\n",
        "    v_shape = volume.shape\n",
        "    out_shape = (v_shape[0], size, size)\n",
        "    mask = resize(\n",
        "        mask,\n",
        "        output_shape=out_shape,\n",
        "        order=0,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    ) #order=0: nearest neighbor\n",
        "    out_shape = out_shape + (v_shape[3],)\n",
        "    volume = resize(\n",
        "        volume,\n",
        "        output_shape=out_shape,\n",
        "        order=2,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    ) #order=2: bi-quadratic\n",
        "    return volume, mask\n",
        "\n",
        "\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "import scipy.misc\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#Logger: tensorflow1で書かれているので、tensorflow2でも動くためにはtf.compat.v1.をつける必要がある\n",
        "class Logger(object):\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag, simple_value=value)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n",
        "\n",
        "    def image_summary(self, tag, image, step):\n",
        "        s = BytesIO()\n",
        "        scipy.misc.toimage(image).save(s, format=\"png\")\n",
        "\n",
        "        # Create an Image object\n",
        "        img_sum = tf.compat.v1.Summary.Image(\n",
        "            encoded_image_string=s.getvalue(),\n",
        "            height=image.shape[0],\n",
        "            width=image.shape[1],\n",
        "        )\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.compat.v1.Summary(value=[tf.compat.v1.Summary.Value(tag=tag, image=img_sum)])\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()\n",
        "\n",
        "    def image_list_summary(self, tag, images, step):\n",
        "        if len(images) == 0:\n",
        "            return\n",
        "        img_summaries = []\n",
        "        for i, img in enumerate(images):\n",
        "            s = BytesIO()\n",
        "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
        "\n",
        "            # Create an Image object\n",
        "            img_sum = tf.compat.v1.Summary.Image(\n",
        "                encoded_image_string=s.getvalue(),\n",
        "                height=img.shape[0],\n",
        "                width=img.shape[1],\n",
        "            )\n",
        "\n",
        "            # Create a Summary value\n",
        "            img_summaries.append(\n",
        "                tf.compat.v1.Summary.Value(tag=\"{}/{}\".format(tag, i), image=img_sum)\n",
        "            )\n",
        "\n",
        "        # Create and write Summary\n",
        "        summary = tf.compat.v1.Summary(value=img_summaries)\n",
        "        self.writer.add_summary(summary, step)\n",
        "        self.writer.flush()"
      ],
      "metadata": {
        "id": "Unh_DrpaikGH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Interference**"
      ],
      "metadata": {
        "id": "hg7C1RlML3R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#これをインポートすれば上のmoduleは不要\n",
        "#from dataset import BrainSegmentationDataset as Dataset\n",
        "\n",
        "\n",
        "#############################\n",
        "weights_dir = \"./weights/unet.pt\" #weightの保存先\n",
        "#############################\n",
        "\n",
        "#predictionsフォルダ作成\n",
        "os.makedirs(\"./predictions\", exist_ok=True)\n",
        "\n",
        "#deviceを定義\n",
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "\n",
        "#データセットとデータローダー\n",
        "dataset = BrainSegmentationDataset(\n",
        "    images_dir=\"./lgg-mri-segmentation/kaggle_3m\", #kaggle dataset使用\n",
        "    subset=\"validation\",\n",
        "    image_size=256,\n",
        "    random_sampling=False,\n",
        ")\n",
        "loader = DataLoader(\n",
        "    dataset, batch_size=32, drop_last=False, num_workers=1\n",
        ")\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "    unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "    \n",
        "    #モデルのweightをロード\n",
        "    state_dict = torch.load(weights_dir, map_location=device)\n",
        "    unet.load_state_dict(state_dict)\n",
        "\n",
        "    unet.eval()\n",
        "    unet.to(device)\n",
        "\n",
        "    input_list = []\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "\n",
        "    for i, data in tqdm(enumerate(loader)):\n",
        "        x, y_true = data\n",
        "        x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "        y_pred = unet(x)\n",
        "        y_pred_np = y_pred.detach().cpu().numpy()\n",
        "        pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
        "\n",
        "        y_true_np = y_true.detach().cpu().numpy()\n",
        "        true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
        "\n",
        "        x_np = x.detach().cpu().numpy()\n",
        "        input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
        "\n",
        "    volumes = postprocess_per_volume(\n",
        "        input_list,\n",
        "        pred_list,\n",
        "        true_list,\n",
        "        loader.dataset.patient_slice_index,\n",
        "        loader.dataset.patients,\n",
        "    )\n",
        "\n",
        "    dsc_dist = dsc_distribution(volumes)\n",
        "\n",
        "    dsc_dist_plot = plot_dsc(dsc_dist)\n",
        "    imsave(\"./dsc.png\", dsc_dist_plot)\n",
        "\n",
        "    for p in volumes:\n",
        "        x = volumes[p][0]\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        for s in range(x.shape[0]):\n",
        "            image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
        "            image = outline(image, y_pred[s, 0], color=[255, 0, 0])\n",
        "            image = outline(image, y_true[s, 0], color=[0, 255, 0])\n",
        "            filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
        "            filepath = os.path.join(\"./predictions\", filename)\n",
        "            imsave(filepath, image)"
      ],
      "metadata": {
        "id": "2vquaAK63hlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prediction結果を表示 ###\n",
        "\n",
        "3列で全てを表示"
      ],
      "metadata": {
        "id": "EIKSAedanJ3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction結果を表示\n",
        "images = [Image.open(img) for img in glob.glob(\"./predictions/*\")[0:30]]\n",
        "\n",
        "cols =3\n",
        "rows = len(images)//cols+1 #縦の行\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(cols*5, rows*5))\n",
        "\n",
        "\n",
        "for i, im in enumerate(images):\n",
        "    fig.add_subplot(rows, cols, i+1).set_title(str(i+1))\n",
        "    plt.imshow(im)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ELO1o_fybS4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train**"
      ],
      "metadata": {
        "id": "7SqUW3wHzdW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from dataset import BrainSegmentationDataset as Dataset #上で定義し直しているのでインポートしない\n",
        "#from logger import Logger\n",
        "from loss import DiceLoss\n",
        "from transform import transforms\n",
        "from unet import UNet\n",
        "from utils import log_images, dsc\n",
        "\n",
        "from statistics import mean\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
        "    dsc_list = []\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index]) #要素の数を数える、すなわち患者毎の枚数をリスト化する\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
        "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
        "        dsc_list.append(dsc(y_pred, y_true))\n",
        "        index += num_slices[p]\n",
        "    return dsc_list\n",
        "\n",
        "\n",
        "def log_loss_summary(logger, loss, step, prefix=\"\"):\n",
        "    logger.scalar_summary(prefix + \"loss\", np.mean(loss), step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def snapshotargs():\n",
        "#     args_file = os.path.join(\"./logs\", \"args.json\")\n",
        "#     with open(args_file, \"w\") as fp:\n",
        "#         json.dump(vars(), fp)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#main\n",
        "\n",
        "#時間測定開始\n",
        "time_start = time.perf_counter()\n",
        "\n",
        "#random_seed\n",
        "np.random.seed = 42\n",
        "\n",
        "#parameters\n",
        "batch_size = 8\n",
        "image_size = 256\n",
        "num_workers = 0\n",
        "lr = 0.0001\n",
        "n_epochs = 100\n",
        "vis_freq = 10 #frequency of saving images to log file\n",
        "vis_images = 200 #number of visualization images to save in log file\n",
        "#weight_path = \"./weights\"\n",
        "weight_dir = \"/content/drive/MyDrive/Kaggle/Brain_segmentation_3dUNET\"\n",
        "load_weight = True #Gdriveに保存しているweightをロードするかどうか\n",
        "\n",
        "#フォルダ作成\n",
        "os.makedirs(\"./weights\", exist_ok=True)\n",
        "os.makedirs(\"./logs\", exist_ok=True)\n",
        "\n",
        "# snapshotargs()\n",
        "\n",
        "#deviceを定義\n",
        "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "\n",
        "\n",
        "if 'loader_valid' in globals(): #データローダー作成済みなら省略する（読み込みに時間かかるので）\n",
        "    pass\n",
        "else:\n",
        "    #dataset\n",
        "    dataset_train = BrainSegmentationDataset(\n",
        "        images_dir=\"./lgg-mri-segmentation/kaggle_3m\",\n",
        "        subset=\"train\",\n",
        "        image_size=image_size,\n",
        "        transform=transforms(scale=0.05, angle=15, flip_prob=0.5), #scale, angle: augmentationの拡大縮小および回転角度\n",
        "    )\n",
        "    dataset_valid = BrainSegmentationDataset(\n",
        "        images_dir=\"./lgg-mri-segmentation/kaggle_3m\",\n",
        "        subset=\"validation\",\n",
        "        image_size=image_size,\n",
        "        random_sampling=False,\n",
        "    )\n",
        "\n",
        "    #dataloader\n",
        "    loader_train = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=num_workers,\n",
        "        worker_init_fn= None,\n",
        "    )\n",
        "    loader_valid = DataLoader(\n",
        "        dataset_valid,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=False,\n",
        "        num_workers=num_workers,\n",
        "        worker_init_fn= None,\n",
        "    )\n",
        "    loaders = {\"train\": loader_train, \"valid\": loader_valid}\n",
        "\n",
        "    print(f\"elapsed_time: {time.perf_counter() - time_start}\")\n",
        "    print(\"\")\n",
        "\n",
        "#model\n",
        "unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "unet.to(device)\n",
        "\n",
        "if load_weight is True:\n",
        "    unet.load_state_dict(torch.load (os.path.join(weight_dir, \"unet.pt\")))\n",
        "    print(\"loading weight...\")\n",
        "else:\n",
        "    pass\n",
        "\n",
        "dsc_loss = DiceLoss()\n",
        "best_validation_dsc = 0.0\n",
        "\n",
        "optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
        "\n",
        "logger = Logger(\"./logs\")\n",
        "loss_train = []\n",
        "loss_valid = []\n",
        "\n",
        "step = 0\n",
        "\n",
        "time_start = time.perf_counter() #時間測定開始\n",
        "for epoch in tqdm(range(n_epochs), total=n_epochs): \n",
        "    for phase in [\"train\", \"valid\"]:\n",
        "        if phase == \"train\":\n",
        "            unet.train()\n",
        "        else:\n",
        "            unet.eval()\n",
        "\n",
        "        validation_pred = []\n",
        "        validation_true = []\n",
        "\n",
        "        for i, data in enumerate(loaders[phase]):\n",
        "            if phase == \"train\":\n",
        "                step += 1\n",
        "\n",
        "            x, y_true = data #x: 入力画像、y_true: ラベル画像\n",
        "            x, y_true = x.to(device), y_true.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == \"train\"):\n",
        "                y_pred = unet(x)\n",
        "\n",
        "                loss = dsc_loss(y_pred, y_true)\n",
        "\n",
        "                if phase == \"valid\":\n",
        "                    loss_valid.append(loss.item())\n",
        "                    val_loss = loss.item() #途中経過表示用\n",
        "                    y_pred_np = y_pred.detach().cpu().numpy()\n",
        "                    validation_pred.extend(\n",
        "                        [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
        "                    )\n",
        "                    y_true_np = y_true.detach().cpu().numpy()\n",
        "                    validation_true.extend(\n",
        "                        [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
        "                    )\n",
        "                    if (epoch % vis_freq == 0) or (epoch == n_epochs - 1):\n",
        "                        if i * batch_size < vis_images:\n",
        "                            tag = \"image/{}\".format(i)\n",
        "                            num_images = vis_images - i * batch_size\n",
        "                            # logger.image_list_summary(\n",
        "                            #     tag,\n",
        "                            #     log_images(x, y_true, y_pred)[:num_images],\n",
        "                            #     step,\n",
        "                            # )\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    loss_train.append(loss.item())\n",
        "                    train_loss = loss.item() #途中経過表示用\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            if phase == \"train\" and (step + 1) % 10 == 0:\n",
        "                #log_loss_summary(logger, loss_train, step)\n",
        "                loss_train = []\n",
        "\n",
        "        if phase == \"valid\":\n",
        "            #log_loss_summary(logger, loss_valid, step, prefix=\"val_\")\n",
        "            mean_dsc = np.mean(\n",
        "                dsc_per_volume(\n",
        "                    validation_pred,\n",
        "                    validation_true,\n",
        "                    loader_valid.dataset.patient_slice_index,\n",
        "                )\n",
        "            )\n",
        "            # logger.scalar_summary(\"val_dsc\", mean_dsc, step)\n",
        "            \n",
        "            ####途中経過####\n",
        "            print(\"\")\n",
        "            print(f\"epoch: {str(epoch+1)}\")\n",
        "            print(f\"train_loss: {train_loss:.5f}\")\n",
        "            print(f\"val_loss: {val_loss:.5f}\")\n",
        "            print(f\"val_dsc: {mean_dsc:.5f}\") \n",
        "            print(f\"elapsed_time: {time.perf_counter() - time_start:.5f}\")\n",
        "            \n",
        "            if mean_dsc > best_validation_dsc:\n",
        "                print(f'mean_dsc increased ({best_validation_dsc:5f} --> {mean_dsc:5f}). Saving model...')\n",
        "                best_validation_dsc = mean_dsc\n",
        "                torch.save(unet.state_dict(), os.path.join(weight_dir, \"unet.pt\"))\n",
        "            print(\"\")\n",
        "            loss_valid = []\n",
        "\n",
        "print(\"Best validation mean DSC: {:4f}\".format(best_validation_dsc))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jR9teGPzfiY",
        "outputId": "1d6b75c0-ffde-47fb-810d-4895f1d3a391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weight...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [01:36<2:39:32, 96.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 1\n",
            "train_loss: 0.24360\n",
            "val_loss: 0.42505\n",
            "val_accuracy: 0.80410\n",
            "elapsed_time: 96.59301\n",
            "mean_dsc increased (0.0 --> 0.8041019030263932). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [03:09<2:34:33, 94.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 2\n",
            "train_loss: 0.25187\n",
            "val_loss: 0.30094\n",
            "val_accuracy: 0.80175\n",
            "elapsed_time: 189.88348\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [04:49<2:36:47, 96.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 3\n",
            "train_loss: 0.11875\n",
            "val_loss: 0.21129\n",
            "val_accuracy: 0.80149\n",
            "elapsed_time: 289.66992\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [06:27<2:35:49, 97.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 4\n",
            "train_loss: 0.10456\n",
            "val_loss: 0.17041\n",
            "val_accuracy: 0.90301\n",
            "elapsed_time: 387.56958\n",
            "mean_dsc increased (0.8041019030263932 --> 0.9030138422825132). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [08:01<2:32:12, 96.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 5\n",
            "train_loss: 0.12307\n",
            "val_loss: 0.14505\n",
            "val_accuracy: 0.81264\n",
            "elapsed_time: 481.57704\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [09:35<2:29:36, 95.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 6\n",
            "train_loss: 0.17300\n",
            "val_loss: 0.12676\n",
            "val_accuracy: 0.81442\n",
            "elapsed_time: 575.83025\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [11:14<2:29:46, 96.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 7\n",
            "train_loss: 0.09998\n",
            "val_loss: 0.11592\n",
            "val_accuracy: 0.81030\n",
            "elapsed_time: 674.79486\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [12:50<2:27:56, 96.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 8\n",
            "train_loss: 0.14251\n",
            "val_loss: 0.10213\n",
            "val_accuracy: 0.81221\n",
            "elapsed_time: 770.95959\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [14:27<2:26:24, 96.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 9\n",
            "train_loss: 0.12919\n",
            "val_loss: 0.09409\n",
            "val_accuracy: 0.89822\n",
            "elapsed_time: 867.61053\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [16:05<2:25:29, 97.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 10\n",
            "train_loss: 0.05479\n",
            "val_loss: 0.09368\n",
            "val_accuracy: 0.90720\n",
            "elapsed_time: 965.54114\n",
            "mean_dsc increased (0.9030138422825132 --> 0.907202934993029). Saving model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [17:42<2:23:54, 97.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 11\n",
            "train_loss: 0.08839\n",
            "val_loss: 0.09682\n",
            "val_accuracy: 0.91215\n",
            "elapsed_time: 1062.62356\n",
            "mean_dsc increased (0.907202934993029 --> 0.9121493135656042). Saving model...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}